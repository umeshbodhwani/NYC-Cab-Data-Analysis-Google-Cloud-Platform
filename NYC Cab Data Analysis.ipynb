{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dates: Array[String] = Array(2017-01, 2017-02, 2017-03, 2017-04, 2017-05, 2017-06, 2017-07, 2017-08, 2017-09, 2017-10, 2017-11, 2017-12)\n",
       "files: Array[String] = Array(taxi/f2017-01.csv, taxi/f2017-02.csv, taxi/f2017-03.csv, taxi/f2017-04.csv, taxi/f2017-05.csv, taxi/f2017-06.csv, taxi/f2017-07.csv, taxi/f2017-08.csv, taxi/f2017-09.csv, taxi/f2017-10.csv, taxi/f2017-11.csv, taxi/f2017-12.csv)\n",
       "data_rdd_array: Array[org.apache.spark.rdd.RDD[String]] = Array(MapPartitionsRDD[90] at mapPartitionsWithIndex at <console>:50, MapPartitionsRDD[93] at mapPartitionsWithIndex at <console>:50, MapPartitionsRDD[96] at mapPartitionsWithIndex at <console>:50, MapPartitionsRDD[99] at mapPartitionsWithIndex at <console>:50, MapPartitionsRDD[102] at mapPartitionsWithIndex at <console>:50, MapPartitionsRDD[1..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "//This code constructs a list of files that need to be read\n",
    "val dates = Array(\"2017-01\",\"2017-02\",\"2017-03\",\"2017-04\",\"2017-05\",\n",
    "                  \"2017-06\",\"2017-07\",\"2017-08\",\"2017-09\",\"2017-10\",\n",
    "                  \"2017-11\",\"2017-12\")\n",
    "val files = dates.map(d => \"gs://cloud-class-2020/taxi/f\"++d++\".csv\") //++ is the concatenation operator\n",
    "//val files = dates.map(d => \"taxi/f\"++d++\".csv\") //++ is the concatenation operator\n",
    "\n",
    "\n",
    "//data_rdd_array is an array of MapPartitionsRDD. Each MapPartitionRDD corresponds to one data file.\n",
    "//Make sure that the data in data_rdd_array has the first two rows dropped (header and a blank row)\n",
    "val data_rdd_array = files.map(f => sc.textFile(f).mapPartitionsWithIndex{ (idx,iter) => if (idx == 0) iter.drop(2) else iter })\n",
    "\n",
    "//all_data combines all the MapPartitionsRDDs in data_rdd_array into a single MapPartitionsRDD\n",
    "//Use union (see next cell) to combine two rdds\n",
    "//Use reduce to combine all the rdds in data_rdd_array\n",
    "val all_data = data_rdd_array.reduce((a,b) => a union b)\n",
    "\n",
    "//Split all rows on comma\n",
    "val split_data = all_data.map(l=>l.split(\",\"))\n",
    "\n",
    "//Create a map rdd. \n",
    "//The key should be the pickup point id (column 7).\n",
    "//The value the sum of the fare (column 10) and the tip (column 13)\n",
    "val mapped_data = split_data.map(l => (l(7),l(10).toDouble + l(13).toDouble))\n",
    "\n",
    "//Write the combiner, the merger, and the mergeAndCombiner\n",
    "\n",
    "val combiner = (x: Double) => (1,x,x*x)\n",
    "val merger = (x: (Int, Double, Double),y: Double) => {\n",
    "  val (c,acc1,acc2) = x\n",
    "  (c +1 , acc1+y,acc2+y*y)\n",
    "}\n",
    "val mergeAndCombiner = (x1: (Int, Double,Double), x2: (Int, Double,Double)) => {\n",
    "  val (c1, acc11, acc12) = x1\n",
    "  val (c2, acc21, acc22) = x2\n",
    "  (c1+c2,acc11+acc21,acc12+acc22)\n",
    "}\n",
    "\n",
    "//Do the combine by key\n",
    "val combined_data = mapped_data.combineByKey(combiner,merger,mergeAndCombiner)\n",
    "\n",
    "//Write a function that returns the variance and the average\n",
    "//getVarAndAvgFunction = > (String, Double, Double)\n",
    "val getVarAndAvgFunction = (x: (String, (Int, Double, Double))) => {\n",
    "  val (identifier, (count, total1, total2)) = x\n",
    "  (identifier,total1/count,(total2 - (total1/count)*(total1/count)*count)/(count-1))\n",
    "}\n",
    "\n",
    "//Collect the result\n",
    "val result = combined_data.map(getVarAndAvgFunction).map(l => (l._1,(l._2,l._3)))\n",
    "\n",
    "val r = result.collectAsMap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: scala.collection.Map[String,(Double, Double)] = Map(188 -> (14.666259597276547,249.3562749761392), 204 -> (55.0072972972973,2740.689309159159), 194 -> (42.80580259365991,915.7370006383576), 90 -> (12.35628407722452,136849.68477359708), 99 -> (52.35909090909092,4817.481649090909), 111 -> (16.039274809160304,274.82378452955464), 167 -> (14.472861247947451,515.6178903314586), 57 -> (24.40454356846473,1113.5467820157687), 210 -> (25.33258503401361,1947.4929134719835), 219 -> (58.66534703196347,2077.871330805105), 84 -> (47.90564102564103,4113.146598920378), 173 -> (13.861785099582002,321.7962916970432), 78 -> (22.57363007778738,2131.890586984146), 63 -> (21.35235922330097,438.77261172130824), 105 -> (20.52487804878049,219.31668748500587), 149 -> (21.416573116691282,634.742579363359), ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[2] at parallelize at <console>:28\n",
       "t2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[3] at parallelize at <console>:29\n",
       "t3: org.apache.spark.rdd.RDD[String] = UnionRDD[4] at union at <console>:30\n",
       "res1: Array[String] = Array(hello, bye, good, fellow)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Example of union\n",
    "val t1 =sc.parallelize(Array(\"hello\",\"bye\"))\n",
    "val t2 = sc.parallelize(Array(\"good\",\"fellow\"))\n",
    "val t3 = t1 union t2\n",
    "t3.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The data</h2>\n",
    "<li>nyc yellow cab data is available from <a href=\"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\">https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page</a></li>\n",
    "<li>We will work with all the data from 2017</li>\n",
    "<li>Rather than downloading each file separately, we'll use a shell script to download all twelve 2017 files</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>get_files.sh</h3>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/bin/bash\n",
    "declare -a arr=(\"2017-01\" \"2017-02\" \"2017-03\" \"2017-04\" \"2017-05\" \"2017-06\" \"2017-07\" \"2017-08\" \"2017-09\" \"2017-10\"\n",
    " \"2017-11\" \"2017-12\" ) \n",
    "for val in ${arr[@]}\n",
    "do\n",
    "        `curl -o /cloud-class-2020/taxi/f$val.csv https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_$val.c\n",
    "sv`\n",
    "done"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LINE 1: #!/bin/bash\n",
    "LINE 2: declare -a arr=(\"2017-01\" \"2017-02\" \"2017-03\" \"2017-04\" \"2017-05\" \"2017-06\" \"2017-07\" \"2017-08\" \"2017-09\" \"2017-10\" \"2017-11\" \"2017-12\" ) \n",
    "LINE 3: for val in ${arr[@]}\n",
    "LINE 4: do\n",
    "LINE 5:        `curl -o /cloud-class-2020/taxi/f$val.csv https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_$val.csv`\n",
    "LINE 6: done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
